{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabioERodrigues/Sentiment-analysis-model-research/blob/main/FinalYear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T43Tm9rNMf6F"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fabio Rodrigues\n",
        "31016067"
      ],
      "metadata": {
        "id": "iI2ih6fH0gZU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxmTx7dgMj6o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import tarfile\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objects as go\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from scipy.stats import uniform\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier,\n",
        "    AdaBoostClassifier\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, GridSearchCV, RandomizedSearchCV\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from transformers import (\n",
        "    BertTokenizer, TFBertForSequenceClassification\n",
        ")\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVup31vixfEC"
      },
      "outputs": [],
      "source": [
        "movies = \"/content/rotten_tomatoes_movies.csv\"\n",
        "movie_review = \"/content/rotten_tomatoes_movie_reviews.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUytb9MuxvF9"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(movies, encoding='latin-1')\n",
        "df2 = pd.read_csv(movie_review, encoding='latin-1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo1GSdfcyDjn"
      },
      "source": [
        "MERGING THE datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DqKGwjwx1OY"
      },
      "outputs": [],
      "source": [
        "# Performs an inner merge on the 'id' column\n",
        "merged_df = pd.merge(df1, df2, on='id', how='inner')\n",
        "\n",
        "# Displays the first few rows of the merged DataFrame\n",
        "print(merged_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJMkEGMAyKiN"
      },
      "source": [
        "#Exploratory Data Analysis(EDA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyMyeap0yPHh"
      },
      "outputs": [],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rqcNfdmySxD"
      },
      "outputs": [],
      "source": [
        "merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-ykhsNjyZEL"
      },
      "outputs": [],
      "source": [
        "merged_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBc1pQWayiub"
      },
      "outputs": [],
      "source": [
        "merged_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JydqboXjyryA"
      },
      "outputs": [],
      "source": [
        "(merged_df.isnull().sum()/(len(merged_df)))*100 # perectange of null values in ecery column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qektn9YdyxxQ"
      },
      "outputs": [],
      "source": [
        "# REMOVING irrelavant COLUMNS\n",
        "\n",
        "columns_to_drop = [\n",
        "     'tomatoMeter', 'ratingContents',\n",
        "    'runtimeMinutes', 'originalLanguage', 'director', 'writer',\n",
        "    'boxOffice', 'distributor', 'soundMix', 'reviewState',\n",
        "    'publicatioName', 'reviewUrl',  'releaseDateTheaters', 'releaseDateStreaming', 'reviewId', 'creationDate', 'criticName', 'rating', 'originalScore'\n",
        "]\n",
        "\n",
        "merged_df = merged_df.drop(columns=columns_to_drop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYcVPWWcz14j"
      },
      "outputs": [],
      "source": [
        "merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGJkxprU06jF"
      },
      "outputs": [],
      "source": [
        "merged_df['reviewText'] = merged_df['reviewText'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wordcloud"
      ],
      "metadata": {
        "id": "KZuIxQ1V61db"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7GxxiDO0e1r"
      },
      "outputs": [],
      "source": [
        "# Concatenate reviews by sentiment\n",
        "negative_reviews = ' '.join(merged_df[merged_df['scoreSentiment'] == \"NEGATIVE\"]['reviewText'])\n",
        "positive_reviews = ' '.join(merged_df[merged_df['scoreSentiment'] == \"POSITIVE\"]['reviewText'])\n",
        "\n",
        "# Generate word clouds\n",
        "wordcloud_neg = WordCloud(width=800, height=400).generate(negative_reviews)\n",
        "wordcloud_pos = WordCloud(width=800, height=400).generate(positive_reviews)\n",
        "\n",
        "# Create subplots: 1 row, 2 columns\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Plot Negative Reviews WordCloud\n",
        "axes[0].imshow(wordcloud_neg, interpolation='bilinear')\n",
        "axes[0].set_title('Class: Negative')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Plot Positive Reviews WordCloud\n",
        "axes[1].imshow(wordcloud_pos, interpolation='bilinear')\n",
        "axes[1].set_title('Class: Positive')\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8AaAz6rv9do"
      },
      "outputs": [],
      "source": [
        "# words like good appear in negative class after looking into it i found a few example i found why this might be for example\n",
        "#\"It doesn't matter if a movie costs 300 million or only 300 dollars; good is good and bad is bad, and Bloodmask: The Possession of Nicole Lameroux is just plain bad.\"\n",
        "#reviews like this might explain why good appears in  negative class\n",
        " #another example\n",
        "#\"In the comic context of A Gorgeous Bird Like Me, a good deal of the savour is lost simply because this femme comes over as much less fatale than may have been intended.\"\n",
        "#Director Joel Schumacher, credited with toning down Ebbe Roe Smith's script, paces the film rivetingly, which goes to show that good film-making doesn't guarantee a good film. Falling Down is as sleek and efficient as a torture chamber.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribution of Review length"
      ],
      "metadata": {
        "id": "hgZfl2lf68bj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZuYD8S11P2c"
      },
      "outputs": [],
      "source": [
        "# Calculate review lengths\n",
        "merged_df['review_length'] = merged_df['reviewText'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Plotting the distribution of review lengths\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(merged_df['review_length'], bins=50, color='c', edgecolor='k', alpha=0.7)\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment distribution"
      ],
      "metadata": {
        "id": "Q3Hxngoo7V-b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEp4IRpkCaOQ"
      },
      "outputs": [],
      "source": [
        "#Sentiment distribution\n",
        "sentiment_counts = merged_df['scoreSentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQFDrcgsozl5"
      },
      "outputs": [],
      "source": [
        "#Sentiment distribution visulaised using pie chart\n",
        "# Pie chart for sentiment distribution\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(\n",
        "    sentiment_counts,\n",
        "    labels=sentiment_counts.index,\n",
        "    autopct='%1.1f%%',\n",
        "    colors=['lightgreen', 'lightcoral'],\n",
        "    startangle=140\n",
        ")\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures the pie is drawn as a circle.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEdu2EVJ1cxD"
      },
      "source": [
        "#Pre-Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YozQJ5CY0vO9"
      },
      "outputs": [],
      "source": [
        "#Removing 0 t0 5 word reviews\n",
        "print(merged_df.shape)\n",
        "# Remove reviews with 5 word or less\n",
        "merged_df = merged_df[merged_df['review_length'] > 5]\n",
        "\n",
        "# Reset index\n",
        "merged_df = merged_df.reset_index(drop=True)\n",
        "\n",
        "# Verify the changes\n",
        "print(merged_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu0z0bxG1tIV"
      },
      "outputs": [],
      "source": [
        "merged_df = merged_df.dropna(subset=['reviewText'])\n",
        "merged_df = merged_df.dropna(subset=['title'])\n",
        "merged_df = merged_df.dropna(subset=['audienceScore'])\n",
        "merged_df = merged_df.dropna(subset=['genre'])\n",
        "merged_df = merged_df.dropna(subset=['scoreSentiment'])\n",
        "\n",
        "# Double-check if any NaN values remain\n",
        "print(merged_df.isnull().sum())\n",
        "# Check if y has NaN values\n",
        "print((merged_df.applymap(lambda x: x.strip() == \"\" if isinstance(x, str) else False)).sum())\n",
        "print((merged_df.applymap(lambda x: x.strip() == \"\" if isinstance(x, str) else False)).sum())\n",
        "print((merged_df.isin([None])).sum())\n",
        "print((merged_df.isin([\"NULL\", \"NA\", \"-\"])).sum())\n",
        "\n",
        "print(merged_df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMUPrWHaJqB3"
      },
      "outputs": [],
      "source": [
        "print(merged_df[\"scoreSentiment\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTf1EIYc1xhr"
      },
      "outputs": [],
      "source": [
        "merged_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cVce7dB1-wb"
      },
      "outputs": [],
      "source": [
        "#Removing special characters for the reviewtext\n",
        "#Preprocess the 'text' column\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#', '', text)  # Remove hashtags symbol\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    return text\n",
        "\n",
        "merged_df['reviewText'] = merged_df['reviewText'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKqc0qin2RSP"
      },
      "outputs": [],
      "source": [
        "#Sample 100,000 to tokenize for model testing\n",
        "\n",
        "sampled_df = merged_df.sample(n=10000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXVoO7b82llE"
      },
      "outputs": [],
      "source": [
        "sampled_df[\"scoreSentiment\"] = sampled_df[\"scoreSentiment\"].map({\"POSITIVE\": 1, \"NEGATIVE\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaxl2WHaIslG"
      },
      "outputs": [],
      "source": [
        "sampled_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoOIZFTzJoMA"
      },
      "outputs": [],
      "source": [
        "print(sampled_df[\"scoreSentiment\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGwgrKQbI9Z_"
      },
      "outputs": [],
      "source": [
        "print(sampled_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0CKJAuKJBvp"
      },
      "outputs": [],
      "source": [
        "sampled_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqnK1zzwp6KQ"
      },
      "source": [
        "#Machine learning methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNRGh7TC3Gvt"
      },
      "outputs": [],
      "source": [
        "#Traditional models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTeHU6jw3lRX"
      },
      "outputs": [],
      "source": [
        "nlp= spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Negation, Lemmatization and tokenisation"
      ],
      "metadata": {
        "id": "btGiqu2Z7nss"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wMAKovo347D"
      },
      "outputs": [],
      "source": [
        "def process_text(s):\n",
        "    doc = nlp(s)\n",
        "    out = []\n",
        "    negation = False\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'neg':  # Detect negation\n",
        "            negation = True\n",
        "        elif token.is_punct:\n",
        "            negation = False\n",
        "\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            lemma = token.lemma_.lower()\n",
        "            if negation:\n",
        "                lemma = f\"not_{lemma}\"\n",
        "                negation = False  # Reset after handling\n",
        "            out.append(lemma)\n",
        "    return ' '.join(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AreUcKLZ3-fi"
      },
      "outputs": [],
      "source": [
        "sampled_df['fltrTxt']= sampled_df['reviewText'].apply(process_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6budbMHMFm22"
      },
      "outputs": [],
      "source": [
        "# after first doing the cod there was overfitting which i found out due to getting 100% accuracy on some of ml models so some changes were made to combat this problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shiymdu74Txa"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X = sampled_df['fltrTxt']\n",
        "y = sampled_df['scoreSentiment']\n",
        "\n",
        "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Now apply TF-IDF vectorization only on the training set and transform the test set\n",
        "vct = TfidfVectorizer(max_features=10_000, stop_words='english', ngram_range=(1,2))\n",
        "X_trn = vct.fit_transform(X_trn)\n",
        "X_tst = vct.transform(X_tst)  # Transform test set with the same vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icoVPSUy4V9M"
      },
      "outputs": [],
      "source": [
        "def model_report(model, verbose=True):\n",
        "    model.fit(X_trn, y_trn)\n",
        "\n",
        "    # Start timing for training\n",
        "    train_start = time.time()\n",
        "    model.fit(X_trn, y_trn)\n",
        "    train_end = time.time()\n",
        "    train_time = train_end - train_start\n",
        "    # Start timing for inference\n",
        "    inference_start = time.time()\n",
        "    y_pred = model.predict(X_tst)\n",
        "    inference_end = time.time()\n",
        "    inference_time = inference_end - inference_start\n",
        "    trnScore = model.score(X_trn, y_trn)\n",
        "    tstScore = model.score(X_tst, y_tst)\n",
        "    cm = confusion_matrix(y_tst, y_pred)\n",
        "    cr = classification_report(y_tst, y_pred)\n",
        "\n",
        "    if verbose:\n",
        "        print('Train Score: %f' % trnScore)\n",
        "        print('Test Score: %f' % tstScore)\n",
        "        print('Training Time: %.2f seconds' % train_time)\n",
        "        print('Inference Time: %.2f seconds' % inference_time)\n",
        "        print('Classification Report:\\n', cr)\n",
        "        ConfusionMatrixDisplay(cm).plot()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "    return {\n",
        "        'trn': trnScore,\n",
        "        'tst': tstScore,\n",
        "        'cm': cm,\n",
        "        'cr': cr,\n",
        "        'train_time': train_time,\n",
        "        'inference_time': inference_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm02AGKv4XxZ"
      },
      "outputs": [],
      "source": [
        "models_dict= {\n",
        "\t'LogisticRegression':     LogisticRegression(max_iter=10_000, class_weight='balanced'),\n",
        "\t'Support Vector':         SVC(class_weight='balanced'),\n",
        "\t'KNeighborsCLassifier':   KNeighborsClassifier(n_neighbors=15), # to fix overfiiting\n",
        "\t'DecisionTreeClassifier': DecisionTreeClassifier(class_weight='balanced', max_depth=10), # Prevent overfitting\n",
        "\t'RandomForestClassifier': RandomForestClassifier(class_weight='balanced'),\n",
        "\t'BaggingClassifier':      BaggingClassifier(),\n",
        "\t'ExtraTreesClassifier':   ExtraTreesClassifier(class_weight='balanced'),\n",
        "\t'AdaBoostClassifier':     AdaBoostClassifier(),\n",
        "\t'XGBClassifier':          XGBClassifier( n_estimators=100, max_depth=3),# Shallow trees help avoid overfitting\n",
        "\t'LGBMClassifier':         LGBMClassifier(class_weight='balanced'),\n",
        "}\n",
        "models= [{'name':k, 'obj':v} for k,v in models_dict.items()]\n",
        "\n",
        "i= 0\n",
        "for model in models:\n",
        "\tprint(\"Evaluating %s...\"%model['name'])\n",
        "\tprint(\"%d/%d models\"%(i, len(models)), end='\\r')\n",
        "\tmodel.update(model_report(model['obj'], verbose=False))\n",
        "\ti+= 1\n",
        "print(\"%d/%d models evaluated\"%(i, len(models)))\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train score and test score for traditional machine learning models"
      ],
      "metadata": {
        "id": "33iCWRRc8OW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8KSSH6y4Z2Y"
      },
      "outputs": [],
      "source": [
        "ml_results_df = pd.DataFrame({\n",
        "\t'Algorithm':           [model['name'] for model in models],\n",
        "\t'Train Score':         [model['trn']  for model in models],\n",
        "\t'Test Score':          [model['tst']  for model in models],\n",
        "\t'train time':          [model['train_time']  for model in models],\n",
        "\t'Inference time':      [model['inference_time']  for model in models],\n",
        "}).set_index('Algorithm').sort_values(by='Test Score', ascending=False)\n",
        "\n",
        "print(\"Traditional ML Models Performance:\")\n",
        "print(ml_results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report"
      ],
      "metadata": {
        "id": "Kj6X9ja98aCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xoDiOIfOiFT"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "\tprint(model['name'])\n",
        "\tprint(model['cr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparemeter tuning for best performing traditional ml models: SVC, logistic regresiion and Extratress classifier"
      ],
      "metadata": {
        "id": "SFFBsTmD8heD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Grid search"
      ],
      "metadata": {
        "id": "-33bfYiI85Rk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk1b0E6lRA04"
      },
      "outputs": [],
      "source": [
        "param_grid_svc = {\n",
        "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
        "    'kernel': ['linear', 'rbf', 'poly'],  # Kernel type\n",
        "    'gamma': ['scale', 'auto']  # Kernel coefficient for ‘rbf’, ‘poly’\n",
        "}\n",
        "\n",
        "grid_svc = GridSearchCV(SVC(), param_grid_svc, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_svc.fit(X_trn, y_trn)\n",
        "\n",
        "print(\"Best parameters for SVC:\", grid_svc.best_params_)\n",
        "print(\"Best SVC Score:\", grid_svc.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nUmaxnzYV98"
      },
      "outputs": [],
      "source": [
        "param_grid_logreg = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],  # Type of regularization\n",
        "    'solver': ['liblinear', 'saga']  # Solver type (for L1, use 'liblinear' or 'saga')\n",
        "}\n",
        "\n",
        "grid_logreg = GridSearchCV(LogisticRegression(max_iter=5000), param_grid_logreg, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_logreg.fit(X_trn, y_trn)\n",
        "\n",
        "print(\"Best parameters for Logistic Regression:\", grid_logreg.best_params_)\n",
        "print(\"Best Logistic Regression Score:\", grid_logreg.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Randomised search"
      ],
      "metadata": {
        "id": "DrwFzesw88hn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkSV9C6RaFyP"
      },
      "outputs": [],
      "source": [
        "param_dist_extratrees = {\n",
        "    'n_estimators': [50, 100, 200],  # Number of trees\n",
        "    'max_depth': [None, 10, 20],  # Maximum depth of trees\n",
        "    'min_samples_split': [2, 5, 10],  # Minimum samples to split\n",
        "    'min_samples_leaf': [1, 2, 4],  # Minimum samples per leaf\n",
        "    'max_features': ['sqrt', 'log2', None]  # Features to consider per split\n",
        "}\n",
        "# Use RandomizedSearchCV\n",
        "random_extratrees = RandomizedSearchCV(\n",
        "    ExtraTreesClassifier(),\n",
        "    param_distributions=param_dist_extratrees,\n",
        "    n_iter=20,  # Number of random parameter combinations to try\n",
        "    cv=3,  # Reduce cross-validation folds to speed up training\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,  # Use all available CPU cores\n",
        "    random_state=42  # For reproducibility\n",
        ")\n",
        "# Fit the model\n",
        "random_extratrees.fit(X_trn, y_trn)\n",
        "# Print best parameters and score\n",
        "print(\"Best parameters for Extra Trees:\", random_extratrees.best_params_)\n",
        "print(\"Best Extra Trees Score:\", random_extratrees.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntf78EneMcy_"
      },
      "source": [
        "#BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHutgUzu-8fy"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cj4wx_kv_Ud9"
      },
      "outputs": [],
      "source": [
        "#Spliting data FIRST to prevent leakage  which happned in the last code\n",
        "# First split: 80% train, 20% test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    sampled_df['reviewText'],\n",
        "    sampled_df['scoreSentiment'],\n",
        "    test_size=0.2,\n",
        "    stratify=sampled_df['scoreSentiment'],\n",
        "    random_state=99\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP4C_Gja_hpG"
      },
      "outputs": [],
      "source": [
        "# Second split: 80% of original -> 80% train (64% total), 20% validation (16% total)\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts,\n",
        "    train_labels,\n",
        "    test_size=0.2,\n",
        "    stratify=train_labels,\n",
        "    random_state=99\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eq2AcjfAHKi"
      },
      "outputs": [],
      "source": [
        "#Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_len = 128\n",
        "\n",
        "# 3. Tokenization function\n",
        "def encode_texts(texts):\n",
        "    return tokenizer(\n",
        "        texts.tolist(),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "# Encode all splits(imp for bert)\n",
        "train_encodings = encode_texts(train_texts)\n",
        "val_encodings = encode_texts(val_texts)\n",
        "test_encodings = encode_texts(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ4cKWupASlP"
      },
      "outputs": [],
      "source": [
        "#Convert to TensorFlow Dataset for better performance\n",
        "def create_dataset(encodings, labels):\n",
        "    return tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask']\n",
        "        },\n",
        "        labels\n",
        "    )).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset = create_dataset(train_encodings, train_labels)\n",
        "val_dataset = create_dataset(val_encodings, val_labels)\n",
        "test_dataset = create_dataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHe6CwO9ALuD"
      },
      "outputs": [],
      "source": [
        "#Model setup\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7QKW3mPBCY7"
      },
      "outputs": [],
      "source": [
        "#Configure training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AlzS9CBFsm"
      },
      "outputs": [],
      "source": [
        "# 7. Train the model\n",
        "# Function to measure training time\n",
        "def time_bert_training(model, train_dataset, val_dataset, epochs=5):\n",
        "    # Start timing for training\n",
        "    train_start = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    train_end = time.time()\n",
        "    train_time = train_end - train_start\n",
        "\n",
        "    print(f'BERT Training Time: {train_time:.2f} seconds')\n",
        "    return history, train_time\n",
        "\n",
        "# Function to measure inference time\n",
        "def time_bert_inference(model, test_dataset):\n",
        "    # Start timing for inference\n",
        "    inference_start = time.time()\n",
        "\n",
        "    # Run prediction\n",
        "    predictions = model.predict(test_dataset)\n",
        "\n",
        "    inference_end = time.time()\n",
        "    inference_time = inference_end - inference_start\n",
        "\n",
        "    # Get the number of test examples\n",
        "    num_examples = sum(batch[1].shape[0] for batch in test_dataset)\n",
        "    per_example_time = inference_time / num_examples\n",
        "\n",
        "    print(f'BERT Inference Time: {inference_time:.2f} seconds')\n",
        "    print(f'Time per example: {per_example_time*1000:.2f} ms')\n",
        "\n",
        "    return predictions, inference_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBjwle8vCDOV"
      },
      "outputs": [],
      "source": [
        "#Full evaluation\n",
        "history, bert_train_time = time_bert_training(model, train_dataset, val_dataset)\n",
        "predictions, bert_inference_time = time_bert_inference(model, test_dataset)\n",
        "y_pred = predictions.logits.argmax(axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels, y_pred))\n",
        "\n",
        "#Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(test_labels, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXjiT1ZkCFua"
      },
      "outputs": [],
      "source": [
        "# After the training code block:\n",
        "\n",
        "# Get scores for FINAL MODEL\n",
        "train_loss, train_acc = model.evaluate(train_dataset, verbose=0)\n",
        "val_loss, val_acc = model.evaluate(val_dataset, verbose=0)\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=0)\n",
        "\n",
        "# Create comprehensive results table\n",
        "results_df = pd.DataFrame({\n",
        "    'Algorithm': ['BERT'],\n",
        "    'Train Score': [train_acc],\n",
        "    'Test Score': [test_acc],\n",
        "    'train time': [bert_train_time],\n",
        "    'Inference time': [bert_inference_time]\n",
        "})\n",
        "print(\"\\nModel Performance:\")\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Optional: Compare with last epoch's training accuracy\n",
        "final_epoch_train_acc = history.history['accuracy'][-1]\n",
        "print(f\"\\nLast Epoch Training Accuracy: {final_epoch_train_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIGmC9E2RPyQ"
      },
      "source": [
        "# Hyper tuning for bert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-R284Qapqbc"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define hyperparameter grid\n",
        "learning_rates = [5e-5, 4e-5, 3e-5, 2e-5]\n",
        "batch_sizes = [16, 32]\n",
        "\n",
        "# Ensure dataset columns exist\n",
        "assert 'reviewText' in sampled_df and 'scoreSentiment' in sampled_df, \"Dataset columns missing!\"\n",
        "\n",
        "# Split data into train / val / test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    sampled_df['reviewText'].tolist(),\n",
        "    sampled_df['scoreSentiment'].tolist(),\n",
        "    test_size=0.2,\n",
        "    stratify=sampled_df['scoreSentiment'],\n",
        "    random_state=99\n",
        ")\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts,\n",
        "    train_labels,\n",
        "    test_size=0.2,\n",
        "    stratify=train_labels,\n",
        "    random_state=99\n",
        ")\n",
        "\n",
        "# Tokenization helper\n",
        "def encode_texts(texts, tokenizer, max_length=128):\n",
        "    return tokenizer(\n",
        "        texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"tf\"\n",
        "    )\n",
        "\n",
        "# Encode all splits\n",
        "train_encodings = encode_texts(train_texts, tokenizer)\n",
        "val_encodings   = encode_texts(val_texts, tokenizer)\n",
        "test_encodings  = encode_texts(test_texts, tokenizer)\n",
        "\n",
        "# Create tf.data.Dataset\n",
        "def create_dataset(encodings, labels, batch_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((\n",
        "        {\n",
        "            'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask']\n",
        "        },\n",
        "        labels\n",
        "    ))\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Grid search over all combinations\n",
        "best_val_accuracy = 0.0\n",
        "best_params = {}\n",
        "trial_num = 0\n",
        "\n",
        "total_trials = len(learning_rates) * len(batch_sizes)\n",
        "print(f\"Starting grid search over {total_trials} trials...\\n\")\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        trial_num += 1\n",
        "        print(f\"Trial {trial_num}/{total_trials}: lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "        # Prepare datasets\n",
        "        train_ds = create_dataset(train_encodings, train_labels, batch_size)\n",
        "        val_ds   = create_dataset(val_encodings,   val_labels,   batch_size)\n",
        "        test_ds  = create_dataset(test_encodings,  test_labels,  batch_size)\n",
        "\n",
        "        # Initialize and compile model\n",
        "        model = TFBertForSequenceClassification.from_pretrained(\n",
        "            \"bert-base-uncased\", num_labels=2\n",
        "        )\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        history = model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=5,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        max_train_acc = max(history.history['accuracy'])\n",
        "        max_val_acc   = max(history.history['val_accuracy'])\n",
        "        test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
        "\n",
        "        print(f\"  --> Train Acc: {max_train_acc:.4f}, Val Acc: {max_val_acc:.4f}, Test Acc: {test_acc:.4f}\\n\")\n",
        "\n",
        "        # Update best\n",
        "        if max_val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = max_val_acc\n",
        "            best_params = {\n",
        "                'learning_rate': lr,\n",
        "                'batch_size': batch_size,\n",
        "                'best_train_accuracy': max_train_acc,\n",
        "                'best_val_accuracy': max_val_acc,\n",
        "                'test_accuracy': test_acc\n",
        "            }\n",
        "\n",
        "# Final results\n",
        "print(\"Grid search complete. Best hyperparameters and performance:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctLRB4j0qpa3"
      },
      "source": [
        "#Comparing traditional methods with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCOoixaYqoXu"
      },
      "outputs": [],
      "source": [
        "# Create one consistent combined dataframe with ALL metrics\n",
        "ml_df = ml_results_df.reset_index().rename(columns={'Algorithm': 'Model'})\n",
        "bert_df = results_df.rename(columns={'Algorithm': 'Model'})\n",
        "#Concatenate with all columns preserved\n",
        "combined_results = pd.concat([ml_df, bert_df], ignore_index=True)\n",
        "print(combined_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxqnyRJLqzDn"
      },
      "outputs": [],
      "source": [
        "# Melt the dataframe to long format for grouped bars\n",
        "melted_results = pd.melt(\n",
        "    combined_results,\n",
        "    id_vars='Model',\n",
        "    value_vars=['Train Score', 'Test Score'],\n",
        "    var_name='Score Type',\n",
        "    value_name='Accuracy'\n",
        ")\n",
        "\n",
        "# Sort models by test accuracy for cleaner order)\n",
        "model_order = combined_results.sort_values(by='Test Score', ascending=False)['Model']\n",
        "melted_results['Model'] = pd.Categorical(melted_results['Model'], categories=model_order, ordered=True)\n",
        "\n",
        "#Plotting\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "bar = sns.barplot(\n",
        "    data=melted_results,\n",
        "    x='Model',\n",
        "    y='Accuracy',\n",
        "    hue='Score Type',\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "# Annotate bars\n",
        "for p in bar.patches:\n",
        "    height = p.get_height()\n",
        "    bar.annotate(\n",
        "        f'{height:.2f}',\n",
        "        (p.get_x() + p.get_width() / 2, height + 0.01),\n",
        "        ha='center',\n",
        "        fontsize=9,\n",
        "        weight='bold'\n",
        "    )\n",
        "\n",
        "# Customize the plot\n",
        "plt.title('Train vs Test Accuracy Comparison of ML Models vs BERT', fontsize=16, weight='bold')\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.legend(title='Score Type', loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparing Genearalisation across all the models"
      ],
      "metadata": {
        "id": "3kZ8P-Fz-huG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute the gap and sort\n",
        "combined_results['Gen. Gap'] = combined_results['Train Score'] - combined_results['Test Score']\n",
        "combined_results = combined_results.set_index('Model').sort_values('Gen. Gap', ascending=False)\n",
        "print(combined_results)"
      ],
      "metadata": {
        "id": "t-GUjZqQD5ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "# Plot generalization gap\n",
        "sns.barplot(data=combined_results.reset_index(), x='Model', y='Gen. Gap', palette='coolwarm')\n",
        "plt.title('Generalization Gap by Model (Train - Test Accuracy)', fontsize=14)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Gap', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "for i, v in enumerate(combined_results['Gen. Gap']):\n",
        "    plt.text(\n",
        "        i,\n",
        "        v + 0.005,\n",
        "        f'{v:.3f}',\n",
        "        ha='center',\n",
        "        fontsize=9\n",
        "    )\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fs_mmk3YEMEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparing compuational time across all the models"
      ],
      "metadata": {
        "id": "d9oXFCInETs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by training time\n",
        "train_time_order = combined_results.sort_values('train time').index.tolist()\n",
        "train_time_df = combined_results.copy()\n",
        "train_time_df['Model'] = pd.Categorical(train_time_df.index, categories=train_time_order, ordered=True)"
      ],
      "metadata": {
        "id": "c7bsUloaEWFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by inference time\n",
        "inference_time_order = combined_results.sort_values('Inference time').index.tolist()\n",
        "inference_time_df = combined_results.copy()\n",
        "inference_time_df['Model'] = pd.Categorical(inference_time_df.index, categories=inference_time_order, ordered=True)"
      ],
      "metadata": {
        "id": "oXR6qUK6EajL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "# subplots\n",
        "fig, (ax2, ax3) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "# Training time plot (log scale)\n",
        "sns.barplot(data=train_time_df, x='Model', y='train time', ax=ax2, palette='viridis')\n",
        "ax2.set_title('Training Time Comparison (Log Scale)', fontsize=14)\n",
        "ax2.set_xlabel('Model', fontsize=12)\n",
        "ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
        "ax2.set_yscale('log')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "for p in ax2.patches:\n",
        "    height = p.get_height()\n",
        "    ax2.annotate(\n",
        "        f'{height:.2f}s',\n",
        "        (p.get_x() + p.get_width() / 2., height*1.05),\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=8\n",
        "    )\n",
        "\n",
        "# Inference time plot (log scale)\n",
        "sns.barplot(data=inference_time_df, x='Model', y='Inference time', ax=ax3, palette='plasma')\n",
        "ax3.set_title('Inference Time Comparison (Log Scale)', fontsize=14)\n",
        "ax3.set_xlabel('Model', fontsize=12)\n",
        "ax3.set_ylabel('Time (seconds)', fontsize=12)\n",
        "ax3.set_yscale('log')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "for p in ax3.patches:\n",
        "    height = p.get_height()\n",
        "    ax3.annotate(\n",
        "        f'{height:.3f}s',\n",
        "        (p.get_x() + p.get_width() / 2., height*1.05),\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=8\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kvjezap9EmL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "combined_results = combined_results.reset_index()\n",
        "\n",
        "# Create efficiency scatter plot\n",
        "plt.scatter(\n",
        "    combined_results['Inference time'],\n",
        "    combined_results['Test Score'],\n",
        "    s=100,\n",
        "    alpha=0.7,\n",
        "    c=np.arange(len(combined_results)),\n",
        "    cmap='viridis'\n",
        ")\n",
        "\n",
        "# Add algorithm labels to the points\n",
        "for i, alg in enumerate(combined_results['Model']):\n",
        "    plt.annotate(\n",
        "        alg,\n",
        "        (combined_results['Inference time'].iloc[i]*1.1, combined_results['Test Score'].iloc[i]),\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Inference Time (seconds) - Log Scale', fontsize=12)\n",
        "plt.ylabel('Test Accuracy', fontsize=12)\n",
        "plt.title('Efficiency Comparison: Accuracy vs Inference Time', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IuatKLS5E1HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWZwFogE-pCs"
      },
      "source": [
        "#Loading BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUjwNl_r-y7V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# === Hyperparameters (Best-found) ===\n",
        "BEST_PARAMS = {\n",
        "    'learning_rate': 3e-5,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 5,\n",
        "    'max_length': 128,\n",
        "    'model_name': 'bert-base-uncased',\n",
        "    'save_dir': './best_bert_model'\n",
        "}\n",
        "\n",
        "\n",
        "df = sampled_df.copy()\n",
        "df['label'] = df['scoreSentiment'].astype(int)\n",
        "\n",
        "# First split: train+val and test\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['reviewText'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
        ")\n",
        "# Second split: train and validation\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
        ")\n",
        "\n",
        "# === Tokenizer and Encoding ===\n",
        "tokenizer = BertTokenizer.from_pretrained(BEST_PARAMS['model_name'])\n",
        "\n",
        "def encode_texts(texts, max_length):\n",
        "    return tokenizer(\n",
        "        texts.tolist(),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "train_enc = encode_texts(train_texts, BEST_PARAMS['max_length'])\n",
        "val_enc   = encode_texts(val_texts, BEST_PARAMS['max_length'])\n",
        "test_enc  = encode_texts(test_texts, BEST_PARAMS['max_length'])\n",
        "\n",
        "# === Create TensorFlow Datasets ===\n",
        "def make_dataset(encodings, labels, batch_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((\n",
        "        {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']},\n",
        "        labels.values\n",
        "    ))\n",
        "    return ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = make_dataset(train_enc, train_labels, BEST_PARAMS['batch_size'])\n",
        "val_ds   = make_dataset(val_enc,   val_labels,   BEST_PARAMS['batch_size'])\n",
        "test_ds  = make_dataset(test_enc,  test_labels,  BEST_PARAMS['batch_size'])\n",
        "\n",
        "# === Build and Compile Model ===\n",
        "model = TFBertForSequenceClassification.from_pretrained(\n",
        "    BEST_PARAMS['model_name'],\n",
        "    num_labels=2\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=BEST_PARAMS['learning_rate'])\n",
        "loss_fn   = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics   = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "# === Train ===\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=BEST_PARAMS['epochs']\n",
        ")\n",
        "\n",
        "# === Evaluate on Test Set ===\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# === Save Model and Tokenizer ===\n",
        "os.makedirs(BEST_PARAMS['save_dir'], exist_ok=True)\n",
        "model.save_pretrained(BEST_PARAMS['save_dir'])\n",
        "tokenizer.save_pretrained(BEST_PARAMS['save_dir'])\n",
        "print(f\"Model and tokenizer saved to: {BEST_PARAMS['save_dir']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Compress the folder into a single .zip\n",
        "!zip -r best_bert_model.zip best_bert_model\n",
        "\n",
        "# 2. Bring up a browser download dialog\n",
        "from google.colab import files\n",
        "files.download('best_bert_model.zip')"
      ],
      "metadata": {
        "id": "Hcgo0gNaHv-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMN/06wMMy/BZnGf7ALmOcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}